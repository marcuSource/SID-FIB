# ğŸ® Aprenentatge per ReforÃ§ - CliffWalking

Aquest repositori contÃ© implementacions de diferents algorismes d'aprenentatge per reforÃ§ per resoldre l'entorn CliffWalking de Gymnasium. Els algorismes implementats sÃ³n:

- **ğŸ§® IteraciÃ³ de Valor (Value Iteration)**: Algorisme basat en programaciÃ³ dinÃ mica que troba la polÃ­tica Ã²ptima de manera iterativa.
- **ğŸ“Š EstimaciÃ³ Directa (Direct Estimation)**: Algorisme basat en el model que aprÃ¨n les probabilitats de transiciÃ³ i les recompenses directament de l'experiÃ¨ncia.
- **ğŸ” Q-Learning**: Algorisme sense model (model-free) que aprÃ¨n els valors Q per a cada parell estat-acciÃ³ mitjanÃ§ant actualitzacions incrementals.
- **ğŸš€ REINFORCE**: Algorisme de gradient de polÃ­tica que optimitza directament la polÃ­tica sense aprendre explÃ­citament els valors Q.


## ğŸ“¥ InstalÂ·laciÃ³

### Requisits

Per executar aquest codi necessitarÃ s Python 3.7 o superior i les segÃ¼ents llibreries:

```bash
pip install gymnasium numpy matplotlib pandas tqdm seaborn
```

### VerificaciÃ³ d'instalÂ·laciÃ³

Pots verificar que l'entorn CliffWalking estÃ  correctament instalÂ·lat executant:

```python
import gymnasium as gym
env = gym.make('CliffWalking-v0', is_slippery=True)
print(f"Nombre d'estats: {env.observation_space.n}")
print(f"Nombre d'accions: {env.action_space.n}")
```

## ğŸ“‚ Estructura del Repositori

```
.
â”œâ”€â”€ value_iteration.py      # ImplementaciÃ³ d'IteraciÃ³ de Valor
â”œâ”€â”€ direct_estimate.py      # ImplementaciÃ³ d'EstimaciÃ³ Directa
â”œâ”€â”€ q_learning.py           # ImplementaciÃ³ de Q-Learning
â”œâ”€â”€ reinforce.py            # ImplementaciÃ³ de REINFORCE
â””â”€â”€ README.md               # Aquest fitxer
```

## ğŸš€ Instruccions d'ExecuciÃ³ i ParametritzaciÃ³

### ğŸ§® IteraciÃ³ de Valor (`value_iteration.py`)

<div style="background-color: #f0f9eb; padding: 10px; border-left: 5px solid #52c41a; margin-bottom: 15px;">
Aquest algorisme calcula iterativament la funciÃ³ de valor i deriva una polÃ­tica Ã²ptima.
</div>

**ExecuciÃ³:**
```bash
python value_iteration.py
```

**Mode d'Ãºs:**
1. Escull una opciÃ³:
   - `1`: Executar experiments complets (amb diferents combinacions de parÃ metres)
   - `2`: Visualitzar resultats existents (si ja has executat experiments prÃ¨viament)
   - `3`: Executar un Ãºnic experiment amb gamma=0.99

**ParÃ metres configurables:**
- `gamma`: Factor de descompte (valor entre 0 i 1). Valors mÃ©s propers a 1 donen mÃ©s importÃ ncia a les recompenses futures.
- `theta`: Llindar de convergÃ¨ncia per aturar l'algorisme.
- `custom_reward`: FunciÃ³ personalitzada per modificar les recompenses de l'entorn.

### ğŸ“Š EstimaciÃ³ Directa (`direct_estimate.py`)

<div style="background-color: #f6ffed; padding: 10px; border-left: 5px solid #73d13d; margin-bottom: 15px;">
Aquest algorisme aprÃ¨n un model de transiciÃ³ i recompensa a partir de l'experiÃ¨ncia, i desprÃ©s utilitza aquest model per calcular la polÃ­tica Ã²ptima.
</div>

**ExecuciÃ³:**
```bash
python direct_estimate.py
```

**Mode d'Ãºs:**
1. Escull una opciÃ³:
   - `1`: Executar experiments complets
   - `2`: Visualitzar resultats existents
   - `3`: Executar un Ãºnic experiment amb configuraciÃ³ base

**ParÃ metres configurables:**
- `gamma`: Factor de descompte (valor entre 0 i 1).
- `planning_steps`: Nombre de passos de planificaciÃ³ per cada pas real.
- `epsilon_start`: Valor inicial d'epsilon per a l'exploraciÃ³.
- `epsilon_end`: Valor mÃ­nim d'epsilon.
- `epsilon_decay`: Factor de decaÃ¯ment d'epsilon.
- `custom_reward`: FunciÃ³ personalitzada per modificar les recompenses.

### ğŸ” Q-Learning (`q_learning.py`)

<div style="background-color: #fff1f0; padding: 10px; border-left: 5px solid #ff4d4f; margin-bottom: 15px;">
Q-Learning Ã©s un algorisme d'aprenentatge per reforÃ§ que aprÃ¨n els valors Q mitjanÃ§ant interaccions directes amb l'entorn.
</div>

**ExecuciÃ³:**
```bash
python q_learning.py
```

**Mode d'Ãºs:**
1. Escull una opciÃ³ (escriu el nÃºmero corresponent):
   - `0`: NomÃ©s visualitzar resultats d'entrenaments previs
   - `1`: Executar un entrenament estÃ ndard
   - Qualsevol altre valor: Executar tots els experiments

**ParÃ metres configurables:**
- `NUM_EPISODES`: Nombre d'episodis a entrenar.
- `GAMMA`: Factor de descompte.
- `LEARNING_RATE`: Taxa d'aprenentatge.
- `EPSILON`: Valor inicial d'epsilon per a l'exploraciÃ³.
- `EPSILON_DECAY`: Factor de decaÃ¯ment d'epsilon.
- `EPSILON_MIN`: Valor mÃ­nim d'epsilon.
- `LEARNING_RATE_DECAY`: Factor de decaÃ¯ment de la taxa d'aprenentatge.

### ğŸš€ REINFORCE (`reinforce.py`)

<div style="background-color: #f0f5ff; padding: 10px; border-left: 5px solid #597ef7; margin-bottom: 15px;">
REINFORCE Ã©s un algorisme de gradient de polÃ­tica que optimitza directament la polÃ­tica sense calcular explÃ­citament els valors Q.
</div>

**ExecuciÃ³:**
```bash
python reinforce.py
```

**Mode d'Ãºs:**
1. Escull una opciÃ³:
   - `1`: Executar experiments complets (3000 episodis)
   - `2`: Visualitzar resultats existents
   - `3`: Executar un Ãºnic experiment amb configuraciÃ³ base (3000 episodis)

**ParÃ metres configurables:**
- `gamma`: Factor de descompte (valor entre 0 i 1).
- `learning_rate`: Taxa d'aprenentatge per a l'optimitzador.
- `num_episodes`: Nombre d'episodis a entrenar.
- `custom_reward`: FunciÃ³ personalitzada per modificar les recompenses.

## ğŸ¯ PersonalitzaciÃ³ de les Recompenses

Tots els algorismes permeten personalitzar la funciÃ³ de recompensa. S'inclouen tres opcions predefinides:

1. `default_reward`: Utilitza la recompensa original sense modificacions.
2. `step_penalty_reward`: Afegeix una petita penalitzaciÃ³ per cada pas per fomentar camins mÃ©s curts.
3. `cliff_avoidance_reward`: Augmenta la penalitzaciÃ³ per caure pel precipici.

## ğŸ“ˆ Visualitzacions

Tots els algorismes inclouen visualitzacions Ãºtils:

- **FunciÃ³ de valor**: Mostra el valor esperat de cada estat.
- **PolÃ­tica**: Mostra la polÃ­tica Ã²ptima apresa.
- **ProgressiÃ³ d'entrenament**: GrÃ fiques de recompensa i passos per episodi.
- **Comparacions de parÃ metres**: GrÃ fiques comparatives dels diferents experiments.

## ğŸ“š ReferÃ¨ncies

- [Gymnasium - CliffWalking](https://gymnasium.farama.org/environments/toy_text/cliff_walking/)
- [Grau en Sistemes d'InformaciÃ³ i Dades (UPC)](https://sites.google.com/upc.edu/grau-sid)

## ğŸ‘¨â€ğŸ’» Autors

- [enric.segarra@estudiantat.upc.edu](mailto:enric.segarra@estudiantat.upc.edu)
- [marc.font.cabarrocas@estudiantat.upc.edu](mailto:marc.font.cabarrocas@estudiantat.upc.edu)
- [pablo.calomardo@estudiantat.upc.edu](mailto:pablo.calomardo@estudiantat.upc.edu)